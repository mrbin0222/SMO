{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SMO with feature point prompts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'clip.model.CLIP'>\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "from segment_anything import build_sam, SamAutomaticMaskGenerator, SamPredictor, sam_model_registry\n",
    "from PIL import Image, ImageDraw\n",
    "import clip\n",
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "from torchvision.transforms import Compose, Resize, ToTensor, Normalize\n",
    "from torchvision.transforms import InterpolationMode\n",
    "BICUBIC = InterpolationMode.BICUBIC\n",
    "\n",
    "import tifffile as tiff\n",
    "import os\n",
    "from sklearn.metrics import jaccard_score\n",
    "\n",
    "import json\n",
    "from collections import defaultdict, Counter\n",
    "import torchvision.datasets as dset\n",
    "from pycocotools.coco import COCO\n",
    "import xml.etree.ElementTree as ET\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import math\n",
    "\n",
    "from yolo11.predict import predict_image\n",
    "from util.common import show_points2, convert_box_xywh_to_xyxy, segment_image, show_anns, show_box_anns\n",
    "from util.itm import retriev\n",
    "from util.loss import get_scores_loss,get_indices_of_values_above_threshold_2\n",
    "from util.reward import get_scores_reward,get_indices_of_values_above_threshold\n",
    "from util.cal_iou import calculate_metrics\n",
    "from util.cal_instance_iou import calculate_dice, calculate_miou\n",
    "from util.nms import NMS\n",
    "from util.featurepoint import edge, get_bz\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading time: 4.85 s\n"
     ]
    }
   ],
   "source": [
    "time_start=time.time()\n",
    "\n",
    "sam_checkpoint = \"../checkpoints/sam_vit_h_4b8939.pth\"\n",
    "model_type = \"vit_h\"\n",
    "\n",
    "device = \"cpu\"\n",
    "\n",
    "sam = sam_model_registry[model_type](checkpoint=sam_checkpoint)\n",
    "sam.to(device=device)\n",
    "\n",
    "mask_generator = SamAutomaticMaskGenerator(sam)\n",
    "print(f\"Loading time: {time.time()-time_start:.2f} s\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load data, e.g. CTC DIC-C2DH-HeLa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_path = \"../datasets/CTC/DIC-C2DH-HeLa/01/t015.tif\"\n",
    "image = cv2.imread(image_path)\n",
    "image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "image0 = cv2.imread(image_path)\n",
    "image0 = cv2.cvtColor(image0, cv2.COLOR_BGR2RGB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# init predictor\n",
    "predictor = SamPredictor(sam)\n",
    "predictor.set_image(image)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Points Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get edge contours\n",
    "cnts = edge(image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# automatically take the number of the most significant edge\n",
    "mini = min(len(p) for p in cnts)\n",
    "# merge the most significant edge\n",
    "mini_batch = []\n",
    "for p in cnts:\n",
    "    p = np.squeeze(p, 1)\n",
    "    mini_batch.append(np.random.permutation(p)[:mini])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare the coordinates and labels\n",
    "input_points = np.concatenate(mini_batch)\n",
    "points = torch.Tensor(input_points).to(predictor.device).unsqueeze(1).view(len(cnts),mini,2)\n",
    "labels = torch.Tensor([int(l) for _, l in input_points]).to(predictor.device).unsqueeze(1).view(len(cnts),mini)\n",
    "transformed_points = predictor.transform.apply_coords_torch(points, image.shape[:2])\n",
    "\n",
    "# predict masks\n",
    "masks_p0, scores_p, logits = predictor.predict_torch(\n",
    "        point_coords=transformed_points,\n",
    "        point_labels=labels,\n",
    "        boxes=None,\n",
    "        multimask_output=False,\n",
    ")\n",
    "masks_p = masks_p0.cpu().detach().numpy()\n",
    "\n",
    "# merge masks\n",
    "masks = []\n",
    "for i,p in enumerate(zip(masks_p,scores_p)):\n",
    "    m = {}\n",
    "    m[\"segmentation\"] = p[0][0]\n",
    "    m[\"area\"] = int(p[0].sum())\n",
    "    a = np.where(p[0][0]==True)\n",
    "    m[\"bbox\"] = [a[1].min(), a[0].min(), a[1].max()-a[1].min(), a[0].max()-a[0].min()]\n",
    "    m[\"predicted_iou\"] = p[1][0].item()\n",
    "    m[\"point_coords\"] = points[i].cpu().detach().numpy().tolist()\n",
    "    m[\"stability_score\"] = p[1][0].item()\n",
    "    m[\"crop_box\"] = [0, 0, image.shape[1], image.shape[0]]\n",
    "    masks.append(m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Calculate points_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "bz = get_bz(cnts,image0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate the number of points in each mask\n",
    "points_count = []\n",
    "for p in masks:\n",
    "    points_count.append(np.count_nonzero(p[\"segmentation\"] & bz))\n",
    "points_count = np.array(points_count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Filter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7: too small\n",
      "9: too small\n",
      "18: too small\n",
      "20: too small\n",
      "21: too small\n",
      "29: too small\n",
      "33: too small\n",
      "35: too small\n",
      "43: too small\n",
      "47: too small\n",
      "54: too small\n",
      "66: too big\n",
      "69: too small\n",
      "78: too small\n",
      "80: too small\n",
      "81: too small\n",
      "86: too small\n",
      "90: too small\n",
      "94: too small\n",
      "96: too small\n",
      "97: too small\n",
      "98: too small\n",
      "99: too small\n",
      "105: too small\n"
     ]
    }
   ],
   "source": [
    "# exclude large, small area items\n",
    "h0,w0,_=image0.shape\n",
    "s0=w0*h0\n",
    "alpha = 0.45\n",
    "gamma = 3e-3\n",
    "h0,w0,_=image0.shape\n",
    "s0=w0*h0\n",
    "masks_f = []\n",
    "for i,p in enumerate(masks):\n",
    "    _,_,w,h = p[\"bbox\"]\n",
    "    if w*h >= alpha*s0:\n",
    "        print(str(i)+': too big')\n",
    "        continue\n",
    "    elif p[\"area\"] >= alpha*s0:\n",
    "        print(str(i)+': too big')\n",
    "        continue\n",
    "    elif p[\"area\"] <= gamma*s0:\n",
    "        print(str(i)+': too small')\n",
    "        continue\n",
    "    else:\n",
    "        masks_f.append(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "masks=masks_f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cut out all masks\n",
    "image = Image.open(image_path)\n",
    "cropped_boxes = []\n",
    "\n",
    "for mask in masks:\n",
    "    # crop masks from input image\n",
    "    cropped_boxes.append(segment_image(image, mask[\"segmentation\"]).crop(convert_box_xywh_to_xyxy(mask[\"bbox\"])))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CLIP ITM Scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = retriev(cropped_boxes, \"HeLa cell\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compute Reward and drop bad ones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores=get_scores_reward(masks,scores,points_count,device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "indices = get_indices_of_values_above_threshold(scores, 0.25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "image = Image.open(image_path)\n",
    "segmentation_masks = []\n",
    "result_crops = []\n",
    "result_masks = []\n",
    "result_scores = []\n",
    "for seg_idx in indices:\n",
    "    segmentation_mask_image = Image.fromarray(masks[seg_idx][\"segmentation\"].astype('uint8') * 255)\n",
    "    result_crop = cropped_boxes[seg_idx]\n",
    "    segmentation_masks.append(segmentation_mask_image)\n",
    "    result_masks.append(masks[seg_idx])\n",
    "    result_scores.append(scores[seg_idx])\n",
    "    result_crops.append(result_crop)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Use NMS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concatenate targets and scores\n",
    "t_scores=torch.tensor(result_scores)\n",
    "b = []\n",
    "for p in result_masks:\n",
    "    xyxy = convert_box_xywh_to_xyxy(p[\"bbox\"])\n",
    "    b.append(xyxy)\n",
    "t_boxes=torch.tensor(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "14"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n = NMS(t_boxes,t_scores,0.25,GIoU=True,eps=1e-7)\n",
    "len(n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select final results\n",
    "n_crops = []\n",
    "n_masks = []\n",
    "n_scores = []\n",
    "for i,v in enumerate(zip(result_crops,result_masks,result_scores)):\n",
    "    if i in list(np.array(n)):\n",
    "        n_crops.append(v[0])\n",
    "        n_masks.append(v[1])\n",
    "        n_scores.append(v[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "original_image = Image.open(image_path)\n",
    "array0 = np.zeros(original_image.size,dtype=bool).T\n",
    "for p in n_masks:\n",
    "    array0 = np.logical_or(array0, p[\"segmentation\"])\n",
    "mat_array0 = np.uint8(array0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load gt\n",
    "gt = tiff.imread(str(\"../datasets/CTC/DIC-C2DH-HeLa/01_ST/SEG/man_seg015.tif\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "gt=np.where(gt>0,np.ones_like(gt),np.zeros_like(gt))\n",
    "img_gt=np.reshape(gt, (gt.shape[0], gt.shape[1])) * 255"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DICE and IoU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "res=mat_array0.copy()\n",
    "gt=gt.reshape(1,gt.shape[0],gt.shape[1])\n",
    "res=res.reshape(1,res.shape[0],res.shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.8240552981709168, 0.7007602456894173)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dice,iou=calculate_metrics(res,gt)\n",
    "dice,iou"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mIoU Score: 0.7506\n"
     ]
    }
   ],
   "source": [
    "instance_miou_score = calculate_miou(res, gt, num_classes=2)\n",
    "print(f\"mIoU Score: {instance_miou_score:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
