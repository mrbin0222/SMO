{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "from PIL import Image\n",
    "from ultralytics import YOLO\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = YOLO(\"./runs/train/train/weights/best.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = model.predict(source=\"./MoNuSegTestData/test/images/\", max_det=9999, iou=0.5)  # Display preds. Accepts all YOLO predict arguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate test performance\n",
    "def evaluate_test_set(results):\n",
    "    # count total nuclei detected\n",
    "    total_nuclei = 0\n",
    "    for result in results:\n",
    "        total_nuclei += len(result.boxes)\n",
    "    \n",
    "    print(f\"test set total image number: {len(results)}\")\n",
    "    print(f\"total nuclei detected: {total_nuclei}\")\n",
    "    print(f\"average nuclei detected per image: {total_nuclei/len(results):.2f}\")\n",
    "    \n",
    "    # calculate average confidence\n",
    "    confidences = []\n",
    "    for result in results:\n",
    "        confidences.extend(result.boxes.conf.tolist())\n",
    "    avg_confidence = sum(confidences) / len(confidences)\n",
    "    print(f\"average confidence: {avg_confidence:.4f}\")\n",
    "    \n",
    "    # visualize detection result distribution\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.hist(confidences, bins=20)\n",
    "    plt.title('detection confidence distribution')\n",
    "    plt.xlabel('confidence')\n",
    "    plt.ylabel('frequency')\n",
    "    plt.show()\n",
    "\n",
    "# start evaluation\n",
    "evaluate_test_set(results)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 0.7957\n",
      "Recall: 0.9101\n",
      "F1-score: 0.8491\n"
     ]
    }
   ],
   "source": [
    "def calculate_metrics(results, label_dir):\n",
    "    # initialize counters\n",
    "    total_gt = 0  # total ground truth boxes\n",
    "    total_det = 0  # total detected boxes\n",
    "    total_tp = 0  # true positives\n",
    "    \n",
    "    # iterate over each image result\n",
    "    for result in results:\n",
    "        # get image name\n",
    "        image_name = result.path.split('/')[-1].replace('.png', '.txt')\n",
    "        label_path = os.path.join(label_dir, image_name)\n",
    "        \n",
    "        # read ground truth\n",
    "        gt_boxes = []\n",
    "        if os.path.exists(label_path):\n",
    "            with open(label_path, 'r') as f:\n",
    "                for line in f:\n",
    "                    # convert YOLO format to xyxy format\n",
    "                    class_id, x_center, y_center, width, height = map(float, line.strip().split())\n",
    "                    x1 = (x_center - width/2) * result.orig_shape[1]\n",
    "                    y1 = (y_center - height/2) * result.orig_shape[0]\n",
    "                    x2 = (x_center + width/2) * result.orig_shape[1]\n",
    "                    y2 = (y_center + height/2) * result.orig_shape[0]\n",
    "                    gt_boxes.append([x1, y1, x2, y2])\n",
    "        \n",
    "        # get detected boxes\n",
    "        det_boxes = result.boxes.xyxy.cpu().numpy()\n",
    "        det_scores = result.boxes.conf.cpu().numpy()\n",
    "        \n",
    "        # calculate IoU and match detected boxes and ground truth boxes\n",
    "        ious = []\n",
    "        for gt_box in gt_boxes:\n",
    "            for det_box in det_boxes:\n",
    "                iou = calculate_iou(gt_box, det_box)\n",
    "                ious.append(iou)\n",
    "        \n",
    "        # use IoU threshold 0.5 to match detected boxes and ground truth boxes\n",
    "        ious = np.array(ious).reshape(len(gt_boxes), len(det_boxes))\n",
    "        matched = np.zeros(len(det_boxes))\n",
    "        tp = 0\n",
    "        \n",
    "        for i in range(len(gt_boxes)):\n",
    "            max_iou = np.max(ious[i])\n",
    "            if max_iou >= 0.5:\n",
    "                max_idx = np.argmax(ious[i])\n",
    "                if not matched[max_idx]:\n",
    "                    tp += 1\n",
    "                    matched[max_idx] = 1\n",
    "        \n",
    "        total_gt += len(gt_boxes)\n",
    "        total_det += len(det_boxes)\n",
    "        total_tp += tp\n",
    "    \n",
    "    # calculate metrics\n",
    "    precision = total_tp / total_det if total_det > 0 else 0\n",
    "    recall = total_tp / total_gt if total_gt > 0 else 0\n",
    "    f1 = 2 * precision * recall / (precision + recall) if (precision + recall) > 0 else 0\n",
    "    \n",
    "    print(f\"Precision: {precision:.4f}\")\n",
    "    print(f\"Recall: {recall:.4f}\")\n",
    "    print(f\"F1-score: {f1:.4f}\")\n",
    "    \n",
    "    return precision, recall, f1\n",
    "\n",
    "def calculate_iou(box1, box2):\n",
    "    # calculate IoU of two boxes\n",
    "    x1 = max(box1[0], box2[0])\n",
    "    y1 = max(box1[1], box2[1])\n",
    "    x2 = min(box1[2], box2[2])\n",
    "    y2 = min(box1[3], box2[3])\n",
    "    \n",
    "    intersection = max(0, x2 - x1) * max(0, y2 - y1)\n",
    "    box1_area = (box1[2] - box1[0]) * (box1[3] - box1[1])\n",
    "    box2_area = (box2[2] - box2[0]) * (box2[3] - box2[1])\n",
    "    union = box1_area + box2_area - intersection\n",
    "    \n",
    "    return intersection / union if union > 0 else 0\n",
    "\n",
    "# calculate test set metrics\n",
    "label_dir = \"./MoNuSegTestData/yololabel/\"\n",
    "precision, recall, f1 = calculate_metrics(results, label_dir)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mAP50: 0.8789\n",
      "mAP50-95: 0.5237\n"
     ]
    }
   ],
   "source": [
    "def calculate_map(results, label_dir, iou_thresholds=None):\n",
    "    \"\"\"\n",
    "    calculate mAP50 and mAP50-95\n",
    "\n",
    "    Args:\n",
    "        results: list of prediction results\n",
    "        label_dir: directory of label files\n",
    "        iou_thresholds: list of IoU thresholds, default is [0.5] and [0.5:0.95:0.05]\n",
    "    \"\"\"\n",
    "    if iou_thresholds is None:\n",
    "        iou_thresholds = [0.5] + list(np.arange(0.5, 1.0, 0.05))\n",
    "    \n",
    "    # store AP for each IoU threshold\n",
    "    aps = []\n",
    "    \n",
    "    for iou_threshold in iou_thresholds:\n",
    "        # store AP for each class\n",
    "        class_aps = []\n",
    "        \n",
    "        # get all detected boxes and ground truth boxes\n",
    "        all_detections = []\n",
    "        all_ground_truths = []\n",
    "        \n",
    "        for result in results:\n",
    "            image_path = result.path.split('/')[-1]\n",
    "            label_path = os.path.join(label_dir, image_path.replace('.png', '.txt'))\n",
    "            \n",
    "            # get predicted boxes\n",
    "            det_boxes = result.boxes.xyxy.cpu().numpy()\n",
    "            det_scores = result.boxes.conf.cpu().numpy()\n",
    "            \n",
    "            # get ground truth boxes\n",
    "            gt_boxes = []\n",
    "            if os.path.exists(label_path):\n",
    "                with open(label_path, 'r') as f:\n",
    "                    for line in f:\n",
    "                        class_id, x_center, y_center, width, height = map(float, line.strip().split())\n",
    "                        # convert to xyxy format\n",
    "                        x1 = (x_center - width/2) * 1000\n",
    "                        y1 = (y_center - height/2) * 1000\n",
    "                        x2 = (x_center + width/2) * 1000\n",
    "                        y2 = (y_center + height/2) * 1000\n",
    "                        gt_boxes.append([x1, y1, x2, y2])\n",
    "            \n",
    "            all_detections.append((det_boxes, det_scores))\n",
    "            all_ground_truths.append(gt_boxes)\n",
    "        \n",
    "        # calculate AP\n",
    "        ap = calculate_ap(all_detections, all_ground_truths, iou_threshold)\n",
    "        aps.append(ap)\n",
    "    \n",
    "    # calculate mAP50 and mAP50-95\n",
    "    map50 = aps[0]  # the first is IoU=0.5 AP\n",
    "    map50_95 = np.mean(aps)  # the average AP of all IoU thresholds\n",
    "    \n",
    "    print(f\"mAP50: {map50:.4f}\")\n",
    "    print(f\"mAP50-95: {map50_95:.4f}\")\n",
    "    \n",
    "    return map50, map50_95\n",
    "\n",
    "def calculate_ap(all_detections, all_ground_truths, iou_threshold):\n",
    "    \"\"\"\n",
    "    calculate AP for a single IoU threshold\n",
    "    \"\"\"\n",
    "    # collect all predicted boxes and scores\n",
    "    all_boxes = []\n",
    "    all_scores = []\n",
    "    all_gt_boxes = []\n",
    "    \n",
    "    for det_boxes, det_scores in all_detections:\n",
    "        all_boxes.extend(det_boxes)\n",
    "        all_scores.extend(det_scores)\n",
    "    \n",
    "    for gt_boxes in all_ground_truths:\n",
    "        all_gt_boxes.extend(gt_boxes)\n",
    "    \n",
    "    # sort by confidence\n",
    "    indices = np.argsort(all_scores)[::-1]\n",
    "    all_boxes = np.array(all_boxes)[indices]\n",
    "    all_scores = np.array(all_scores)[indices]\n",
    "    \n",
    "    # calculate precision and recall\n",
    "    tp = np.zeros(len(all_boxes))\n",
    "    fp = np.zeros(len(all_boxes))\n",
    "    gt_matched = np.zeros(len(all_gt_boxes))\n",
    "    \n",
    "    for i, box in enumerate(all_boxes):\n",
    "        max_iou = 0\n",
    "        max_idx = -1\n",
    "        \n",
    "        for j, gt_box in enumerate(all_gt_boxes):\n",
    "            if not gt_matched[j]:\n",
    "                iou = calculate_iou(box, gt_box)\n",
    "                if iou > max_iou:\n",
    "                    max_iou = iou\n",
    "                    max_idx = j\n",
    "        \n",
    "        if max_iou >= iou_threshold:\n",
    "            tp[i] = 1\n",
    "            gt_matched[max_idx] = 1\n",
    "        else:\n",
    "            fp[i] = 1\n",
    "    \n",
    "    # calculate cumulative values\n",
    "    tp_cumsum = np.cumsum(tp)\n",
    "    fp_cumsum = np.cumsum(fp)\n",
    "    \n",
    "    # calculate precision and recall\n",
    "    precision = tp_cumsum / (tp_cumsum + fp_cumsum)\n",
    "    recall = tp_cumsum / len(all_gt_boxes)\n",
    "    \n",
    "    # calculate AP (using 11-point interpolation)\n",
    "    ap = 0\n",
    "    for t in np.arange(0, 1.1, 0.1):\n",
    "        if np.sum(recall >= t) == 0:\n",
    "            p = 0\n",
    "        else:\n",
    "            p = np.max(precision[recall >= t])\n",
    "        ap = ap + p / 11.0\n",
    "    \n",
    "    return ap\n",
    "\n",
    "# calculate mAP50 and mAP50-95\n",
    "map50, map50_95 = calculate_map(results, label_dir)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
